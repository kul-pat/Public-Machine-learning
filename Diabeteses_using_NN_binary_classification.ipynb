{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Diabeteses using NN binary classification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMKixLaubbT5Dx65z/jFnlu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kul-pat/Public-Machine-learning/blob/main/Diabeteses_using_NN_binary_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXPjuCAB9wZ0"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "pijmqVIz-Tnj",
        "outputId": "dd97514d-8cc1-40f4-c5fa-ca6c45f84563"
      },
      "source": [
        "df = pd.read_csv('diabetes.csv')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Pregnancies  Glucose  BloodPressure  ...  DiabetesPedigreeFunction  Age  Outcome\n",
              "0            6      148             72  ...                     0.627   50        1\n",
              "1            1       85             66  ...                     0.351   31        0\n",
              "2            8      183             64  ...                     0.672   32        1\n",
              "3            1       89             66  ...                     0.167   21        0\n",
              "4            0      137             40  ...                     2.288   33        1\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "B4FiL9S--tpw",
        "outputId": "032183bd-3b30-4e4c-e1fa-1b0064daf6c2"
      },
      "source": [
        "df['Glucose']=np.where(df['Glucose']==0,df['Glucose'].median(),df['Glucose'])\n",
        "df['Insulin']=np.where(df['Insulin']==0,df['Insulin'].median(),df['Insulin'])\n",
        "df['SkinThickness']=np.where(df['SkinThickness']==0,df['SkinThickness'].median(),df['SkinThickness'])\n",
        "df['BloodPressure']=np.where(df['BloodPressure']==0,df['BloodPressure'].median(),df['BloodPressure'])\n",
        "df['BMI']=np.where(df['BMI']==0,df['BMI'].median(),df['BMI'])\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>30.5</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>30.5</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183.0</td>\n",
              "      <td>64.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>30.5</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>168.0</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Pregnancies  Glucose  BloodPressure  ...  DiabetesPedigreeFunction  Age  Outcome\n",
              "0            6    148.0           72.0  ...                     0.627   50        1\n",
              "1            1     85.0           66.0  ...                     0.351   31        0\n",
              "2            8    183.0           64.0  ...                     0.672   32        1\n",
              "3            1     89.0           66.0  ...                     0.167   21        0\n",
              "4            0    137.0           40.0  ...                     2.288   33        1\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_-shHSs-lVT"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix,classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "fd_voDQ__aeW",
        "outputId": "793e59a1-c2f8-4787-8b41-4c6b674d3e60"
      },
      "source": [
        "sns.countplot(x='Outcome',data=df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f1672b96890>"
            ]
          },
          "metadata": {},
          "execution_count": 117
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwojIRG5_vX6"
      },
      "source": [
        "X= df.iloc[:,0:-1].values\n",
        "y = df.iloc[:,-1].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ROI7rb-AC2R",
        "outputId": "54f25a45-d2c3-4f85-f6e5-d900f28693aa"
      },
      "source": [
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=0)\n",
        "print(type(X_train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbUoOV60AU-Z"
      },
      "source": [
        "#scaler = StandardScaler()\n",
        "#X_train=scaler.fit_transform(X_train)\n",
        "#X_test = scaler.fit_transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-5i7EhyA7Hp"
      },
      "source": [
        "Epochs = 1000\n",
        "Batch_size = 50\n",
        "Learnig_rate = 0.001"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Rseua7LGwMm"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ir3g-rucBTMk"
      },
      "source": [
        "## train data\n",
        "class TrainData(Dataset):\n",
        "    \n",
        "    def __init__(self, X_data, y_data):\n",
        "        self.X_data = X_data\n",
        "        self.y_data = y_data\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        return self.X_data[index], self.y_data[index]\n",
        "        \n",
        "    def __len__ (self):\n",
        "        return len(self.X_data)\n",
        "\n",
        "\n",
        "train_data = TrainData(torch.FloatTensor(X_train), \n",
        "                       torch.FloatTensor(y_train))\n",
        " ## test data    \n",
        "class TestData(Dataset):\n",
        "    \n",
        "    def __init__(self, X_data):\n",
        "        self.X_data = X_data\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        return self.X_data[index]\n",
        "        \n",
        "    def __len__ (self):\n",
        "        return len(self.X_data)\n",
        "    \n",
        "\n",
        "test_data = TestData(torch.FloatTensor(X_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXiH3lNDMl4l"
      },
      "source": [
        "train_loader = DataLoader(dataset=train_data, batch_size=Batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_data, batch_size=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ot5y1xtPM0Ce"
      },
      "source": [
        "class BinaryClassification(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BinaryClassification, self).__init__()\n",
        "        # Number of input features is 8.\n",
        "        self.layer_1 = nn.Linear(8, 64) \n",
        "        self.layer_2 = nn.Linear(64, 64)\n",
        "        self.layer_out = nn.Linear(64, 1) \n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "        self.batchnorm1 = nn.BatchNorm1d(64)\n",
        "        self.batchnorm2 = nn.BatchNorm1d(64)\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        x = self.relu(self.layer_1(inputs))\n",
        "        x = self.batchnorm1(x)\n",
        "        x = self.relu(self.layer_2(x))\n",
        "        x = self.batchnorm2(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.layer_out(x)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2M0ZJVqNIY5",
        "outputId": "bb002bb3-bebf-4995-855d-4e3e913e0a2e"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNppOD5pNZ40",
        "outputId": "bb7f1e01-e691-46fd-c034-5fece241e63e"
      },
      "source": [
        "model = BinaryClassification()\n",
        "model.to(device)\n",
        "print(model)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=Learnig_rate)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BinaryClassification(\n",
            "  (layer_1): Linear(in_features=8, out_features=64, bias=True)\n",
            "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRViJGFFNxzZ"
      },
      "source": [
        "def binary_acc(y_pred, y_test):\n",
        "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
        "\n",
        "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
        "    acc = correct_results_sum/y_test.shape[0]\n",
        "    acc = torch.round(acc * 100)\n",
        "    \n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdf8guuKN7u-",
        "outputId": "94d591c4-83b6-4121-bfd7-777c4f40b06a"
      },
      "source": [
        "model.train()\n",
        "for e in range(1, Epochs+1):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        y_pred = model(X_batch)\n",
        "        \n",
        "        loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
        "        acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "\n",
        "    print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001: | Loss: 0.68162 | Acc: 56.077\n",
            "Epoch 002: | Loss: 0.55287 | Acc: 68.846\n",
            "Epoch 003: | Loss: 0.53648 | Acc: 72.385\n",
            "Epoch 004: | Loss: 0.51545 | Acc: 74.846\n",
            "Epoch 005: | Loss: 0.49967 | Acc: 75.154\n",
            "Epoch 006: | Loss: 0.51435 | Acc: 73.077\n",
            "Epoch 007: | Loss: 0.48708 | Acc: 74.923\n",
            "Epoch 008: | Loss: 0.48760 | Acc: 77.462\n",
            "Epoch 009: | Loss: 0.48591 | Acc: 77.692\n",
            "Epoch 010: | Loss: 0.46190 | Acc: 77.154\n",
            "Epoch 011: | Loss: 0.46822 | Acc: 77.692\n",
            "Epoch 012: | Loss: 0.48518 | Acc: 76.615\n",
            "Epoch 013: | Loss: 0.47160 | Acc: 77.615\n",
            "Epoch 014: | Loss: 0.46477 | Acc: 76.923\n",
            "Epoch 015: | Loss: 0.45943 | Acc: 77.538\n",
            "Epoch 016: | Loss: 0.44329 | Acc: 77.615\n",
            "Epoch 017: | Loss: 0.45554 | Acc: 77.000\n",
            "Epoch 018: | Loss: 0.44957 | Acc: 77.923\n",
            "Epoch 019: | Loss: 0.44482 | Acc: 79.769\n",
            "Epoch 020: | Loss: 0.44303 | Acc: 78.077\n",
            "Epoch 021: | Loss: 0.45081 | Acc: 77.615\n",
            "Epoch 022: | Loss: 0.44879 | Acc: 78.692\n",
            "Epoch 023: | Loss: 0.42444 | Acc: 78.692\n",
            "Epoch 024: | Loss: 0.45848 | Acc: 78.538\n",
            "Epoch 025: | Loss: 0.42806 | Acc: 79.692\n",
            "Epoch 026: | Loss: 0.42880 | Acc: 79.692\n",
            "Epoch 027: | Loss: 0.44428 | Acc: 81.308\n",
            "Epoch 028: | Loss: 0.43680 | Acc: 77.846\n",
            "Epoch 029: | Loss: 0.43268 | Acc: 78.615\n",
            "Epoch 030: | Loss: 0.43790 | Acc: 78.154\n",
            "Epoch 031: | Loss: 0.43134 | Acc: 78.846\n",
            "Epoch 032: | Loss: 0.44770 | Acc: 77.615\n",
            "Epoch 033: | Loss: 0.43783 | Acc: 79.308\n",
            "Epoch 034: | Loss: 0.42415 | Acc: 78.462\n",
            "Epoch 035: | Loss: 0.45118 | Acc: 78.077\n",
            "Epoch 036: | Loss: 0.42868 | Acc: 80.231\n",
            "Epoch 037: | Loss: 0.43595 | Acc: 77.923\n",
            "Epoch 038: | Loss: 0.40964 | Acc: 80.538\n",
            "Epoch 039: | Loss: 0.42517 | Acc: 81.385\n",
            "Epoch 040: | Loss: 0.43457 | Acc: 78.923\n",
            "Epoch 041: | Loss: 0.42518 | Acc: 80.846\n",
            "Epoch 042: | Loss: 0.41665 | Acc: 79.615\n",
            "Epoch 043: | Loss: 0.40987 | Acc: 80.308\n",
            "Epoch 044: | Loss: 0.43414 | Acc: 81.077\n",
            "Epoch 045: | Loss: 0.40898 | Acc: 81.308\n",
            "Epoch 046: | Loss: 0.45014 | Acc: 78.692\n",
            "Epoch 047: | Loss: 0.43548 | Acc: 79.154\n",
            "Epoch 048: | Loss: 0.44015 | Acc: 80.077\n",
            "Epoch 049: | Loss: 0.43340 | Acc: 78.154\n",
            "Epoch 050: | Loss: 0.42638 | Acc: 79.231\n",
            "Epoch 051: | Loss: 0.46830 | Acc: 77.077\n",
            "Epoch 052: | Loss: 0.41600 | Acc: 79.923\n",
            "Epoch 053: | Loss: 0.40439 | Acc: 81.692\n",
            "Epoch 054: | Loss: 0.43159 | Acc: 79.077\n",
            "Epoch 055: | Loss: 0.41349 | Acc: 80.308\n",
            "Epoch 056: | Loss: 0.42398 | Acc: 79.846\n",
            "Epoch 057: | Loss: 0.41471 | Acc: 80.692\n",
            "Epoch 058: | Loss: 0.39683 | Acc: 81.308\n",
            "Epoch 059: | Loss: 0.43315 | Acc: 79.769\n",
            "Epoch 060: | Loss: 0.42436 | Acc: 80.923\n",
            "Epoch 061: | Loss: 0.40312 | Acc: 80.923\n",
            "Epoch 062: | Loss: 0.41626 | Acc: 80.154\n",
            "Epoch 063: | Loss: 0.40924 | Acc: 80.231\n",
            "Epoch 064: | Loss: 0.42839 | Acc: 80.923\n",
            "Epoch 065: | Loss: 0.43144 | Acc: 80.000\n",
            "Epoch 066: | Loss: 0.39014 | Acc: 80.923\n",
            "Epoch 067: | Loss: 0.40861 | Acc: 82.077\n",
            "Epoch 068: | Loss: 0.39795 | Acc: 81.615\n",
            "Epoch 069: | Loss: 0.41981 | Acc: 81.846\n",
            "Epoch 070: | Loss: 0.41568 | Acc: 80.154\n",
            "Epoch 071: | Loss: 0.41012 | Acc: 80.923\n",
            "Epoch 072: | Loss: 0.40309 | Acc: 81.769\n",
            "Epoch 073: | Loss: 0.40756 | Acc: 82.000\n",
            "Epoch 074: | Loss: 0.39699 | Acc: 81.769\n",
            "Epoch 075: | Loss: 0.40763 | Acc: 81.231\n",
            "Epoch 076: | Loss: 0.41052 | Acc: 79.615\n",
            "Epoch 077: | Loss: 0.38586 | Acc: 81.615\n",
            "Epoch 078: | Loss: 0.43380 | Acc: 79.077\n",
            "Epoch 079: | Loss: 0.40845 | Acc: 82.385\n",
            "Epoch 080: | Loss: 0.41928 | Acc: 80.154\n",
            "Epoch 081: | Loss: 0.40342 | Acc: 82.615\n",
            "Epoch 082: | Loss: 0.43441 | Acc: 78.769\n",
            "Epoch 083: | Loss: 0.41446 | Acc: 78.462\n",
            "Epoch 084: | Loss: 0.40363 | Acc: 81.231\n",
            "Epoch 085: | Loss: 0.40204 | Acc: 82.231\n",
            "Epoch 086: | Loss: 0.39820 | Acc: 80.077\n",
            "Epoch 087: | Loss: 0.39679 | Acc: 80.846\n",
            "Epoch 088: | Loss: 0.38676 | Acc: 83.308\n",
            "Epoch 089: | Loss: 0.38266 | Acc: 81.077\n",
            "Epoch 090: | Loss: 0.39938 | Acc: 81.462\n",
            "Epoch 091: | Loss: 0.38508 | Acc: 81.769\n",
            "Epoch 092: | Loss: 0.41426 | Acc: 80.308\n",
            "Epoch 093: | Loss: 0.37623 | Acc: 83.154\n",
            "Epoch 094: | Loss: 0.41107 | Acc: 80.692\n",
            "Epoch 095: | Loss: 0.37905 | Acc: 81.385\n",
            "Epoch 096: | Loss: 0.38204 | Acc: 82.769\n",
            "Epoch 097: | Loss: 0.40219 | Acc: 82.385\n",
            "Epoch 098: | Loss: 0.43152 | Acc: 79.308\n",
            "Epoch 099: | Loss: 0.38558 | Acc: 82.692\n",
            "Epoch 100: | Loss: 0.38217 | Acc: 80.846\n",
            "Epoch 101: | Loss: 0.40449 | Acc: 79.538\n",
            "Epoch 102: | Loss: 0.39244 | Acc: 81.615\n",
            "Epoch 103: | Loss: 0.41263 | Acc: 79.154\n",
            "Epoch 104: | Loss: 0.40559 | Acc: 79.462\n",
            "Epoch 105: | Loss: 0.37402 | Acc: 83.000\n",
            "Epoch 106: | Loss: 0.38543 | Acc: 80.692\n",
            "Epoch 107: | Loss: 0.38079 | Acc: 82.077\n",
            "Epoch 108: | Loss: 0.41217 | Acc: 80.462\n",
            "Epoch 109: | Loss: 0.40133 | Acc: 81.692\n",
            "Epoch 110: | Loss: 0.44048 | Acc: 78.231\n",
            "Epoch 111: | Loss: 0.38674 | Acc: 82.538\n",
            "Epoch 112: | Loss: 0.38648 | Acc: 83.077\n",
            "Epoch 113: | Loss: 0.39937 | Acc: 81.385\n",
            "Epoch 114: | Loss: 0.36379 | Acc: 84.231\n",
            "Epoch 115: | Loss: 0.39443 | Acc: 82.308\n",
            "Epoch 116: | Loss: 0.39993 | Acc: 79.769\n",
            "Epoch 117: | Loss: 0.40003 | Acc: 79.769\n",
            "Epoch 118: | Loss: 0.40281 | Acc: 79.385\n",
            "Epoch 119: | Loss: 0.38710 | Acc: 83.692\n",
            "Epoch 120: | Loss: 0.37810 | Acc: 82.231\n",
            "Epoch 121: | Loss: 0.39055 | Acc: 81.462\n",
            "Epoch 122: | Loss: 0.38144 | Acc: 82.692\n",
            "Epoch 123: | Loss: 0.37012 | Acc: 83.231\n",
            "Epoch 124: | Loss: 0.39449 | Acc: 83.538\n",
            "Epoch 125: | Loss: 0.36960 | Acc: 82.538\n",
            "Epoch 126: | Loss: 0.39779 | Acc: 82.692\n",
            "Epoch 127: | Loss: 0.38350 | Acc: 82.846\n",
            "Epoch 128: | Loss: 0.39656 | Acc: 80.923\n",
            "Epoch 129: | Loss: 0.37764 | Acc: 84.308\n",
            "Epoch 130: | Loss: 0.36262 | Acc: 82.538\n",
            "Epoch 131: | Loss: 0.35201 | Acc: 84.308\n",
            "Epoch 132: | Loss: 0.36352 | Acc: 82.231\n",
            "Epoch 133: | Loss: 0.37773 | Acc: 82.385\n",
            "Epoch 134: | Loss: 0.36333 | Acc: 82.923\n",
            "Epoch 135: | Loss: 0.37265 | Acc: 82.769\n",
            "Epoch 136: | Loss: 0.39123 | Acc: 81.923\n",
            "Epoch 137: | Loss: 0.36172 | Acc: 83.538\n",
            "Epoch 138: | Loss: 0.36793 | Acc: 83.308\n",
            "Epoch 139: | Loss: 0.37561 | Acc: 81.769\n",
            "Epoch 140: | Loss: 0.34613 | Acc: 84.846\n",
            "Epoch 141: | Loss: 0.37619 | Acc: 81.538\n",
            "Epoch 142: | Loss: 0.37778 | Acc: 82.077\n",
            "Epoch 143: | Loss: 0.38267 | Acc: 83.154\n",
            "Epoch 144: | Loss: 0.34063 | Acc: 83.923\n",
            "Epoch 145: | Loss: 0.38746 | Acc: 82.385\n",
            "Epoch 146: | Loss: 0.36876 | Acc: 82.846\n",
            "Epoch 147: | Loss: 0.39115 | Acc: 81.154\n",
            "Epoch 148: | Loss: 0.37194 | Acc: 82.846\n",
            "Epoch 149: | Loss: 0.38178 | Acc: 80.692\n",
            "Epoch 150: | Loss: 0.34351 | Acc: 84.077\n",
            "Epoch 151: | Loss: 0.35655 | Acc: 83.615\n",
            "Epoch 152: | Loss: 0.38972 | Acc: 81.462\n",
            "Epoch 153: | Loss: 0.36157 | Acc: 83.923\n",
            "Epoch 154: | Loss: 0.34479 | Acc: 83.231\n",
            "Epoch 155: | Loss: 0.35793 | Acc: 81.615\n",
            "Epoch 156: | Loss: 0.43966 | Acc: 81.615\n",
            "Epoch 157: | Loss: 0.36381 | Acc: 82.923\n",
            "Epoch 158: | Loss: 0.35735 | Acc: 83.538\n",
            "Epoch 159: | Loss: 0.37662 | Acc: 83.000\n",
            "Epoch 160: | Loss: 0.36798 | Acc: 81.615\n",
            "Epoch 161: | Loss: 0.35147 | Acc: 83.846\n",
            "Epoch 162: | Loss: 0.36721 | Acc: 82.077\n",
            "Epoch 163: | Loss: 0.35344 | Acc: 82.769\n",
            "Epoch 164: | Loss: 0.36926 | Acc: 81.308\n",
            "Epoch 165: | Loss: 0.34724 | Acc: 83.308\n",
            "Epoch 166: | Loss: 0.34894 | Acc: 82.385\n",
            "Epoch 167: | Loss: 0.34155 | Acc: 84.538\n",
            "Epoch 168: | Loss: 0.35245 | Acc: 83.769\n",
            "Epoch 169: | Loss: 0.36312 | Acc: 83.846\n",
            "Epoch 170: | Loss: 0.34585 | Acc: 85.000\n",
            "Epoch 171: | Loss: 0.33833 | Acc: 84.538\n",
            "Epoch 172: | Loss: 0.33396 | Acc: 85.769\n",
            "Epoch 173: | Loss: 0.33860 | Acc: 82.308\n",
            "Epoch 174: | Loss: 0.34938 | Acc: 84.462\n",
            "Epoch 175: | Loss: 0.36207 | Acc: 82.462\n",
            "Epoch 176: | Loss: 0.35387 | Acc: 83.308\n",
            "Epoch 177: | Loss: 0.34725 | Acc: 84.077\n",
            "Epoch 178: | Loss: 0.33469 | Acc: 85.462\n",
            "Epoch 179: | Loss: 0.34359 | Acc: 84.846\n",
            "Epoch 180: | Loss: 0.35765 | Acc: 85.385\n",
            "Epoch 181: | Loss: 0.33706 | Acc: 84.462\n",
            "Epoch 182: | Loss: 0.36117 | Acc: 82.462\n",
            "Epoch 183: | Loss: 0.34801 | Acc: 84.538\n",
            "Epoch 184: | Loss: 0.35705 | Acc: 81.077\n",
            "Epoch 185: | Loss: 0.34370 | Acc: 83.462\n",
            "Epoch 186: | Loss: 0.32552 | Acc: 83.538\n",
            "Epoch 187: | Loss: 0.35557 | Acc: 83.077\n",
            "Epoch 188: | Loss: 0.33141 | Acc: 84.385\n",
            "Epoch 189: | Loss: 0.31418 | Acc: 86.308\n",
            "Epoch 190: | Loss: 0.34712 | Acc: 83.000\n",
            "Epoch 191: | Loss: 0.32116 | Acc: 86.077\n",
            "Epoch 192: | Loss: 0.34460 | Acc: 83.923\n",
            "Epoch 193: | Loss: 0.37013 | Acc: 83.615\n",
            "Epoch 194: | Loss: 0.34405 | Acc: 83.231\n",
            "Epoch 195: | Loss: 0.35630 | Acc: 83.154\n",
            "Epoch 196: | Loss: 0.33743 | Acc: 84.769\n",
            "Epoch 197: | Loss: 0.32415 | Acc: 85.923\n",
            "Epoch 198: | Loss: 0.32604 | Acc: 83.692\n",
            "Epoch 199: | Loss: 0.31936 | Acc: 85.462\n",
            "Epoch 200: | Loss: 0.32434 | Acc: 85.462\n",
            "Epoch 201: | Loss: 0.31090 | Acc: 86.231\n",
            "Epoch 202: | Loss: 0.32710 | Acc: 84.231\n",
            "Epoch 203: | Loss: 0.36113 | Acc: 83.385\n",
            "Epoch 204: | Loss: 0.34284 | Acc: 84.846\n",
            "Epoch 205: | Loss: 0.34156 | Acc: 82.846\n",
            "Epoch 206: | Loss: 0.34571 | Acc: 85.923\n",
            "Epoch 207: | Loss: 0.32914 | Acc: 85.538\n",
            "Epoch 208: | Loss: 0.35895 | Acc: 82.538\n",
            "Epoch 209: | Loss: 0.33142 | Acc: 84.000\n",
            "Epoch 210: | Loss: 0.33565 | Acc: 82.846\n",
            "Epoch 211: | Loss: 0.31829 | Acc: 85.615\n",
            "Epoch 212: | Loss: 0.36279 | Acc: 83.615\n",
            "Epoch 213: | Loss: 0.33924 | Acc: 84.231\n",
            "Epoch 214: | Loss: 0.37556 | Acc: 82.308\n",
            "Epoch 215: | Loss: 0.35615 | Acc: 82.385\n",
            "Epoch 216: | Loss: 0.34160 | Acc: 84.000\n",
            "Epoch 217: | Loss: 0.32112 | Acc: 85.154\n",
            "Epoch 218: | Loss: 0.32813 | Acc: 84.846\n",
            "Epoch 219: | Loss: 0.31429 | Acc: 87.154\n",
            "Epoch 220: | Loss: 0.32196 | Acc: 85.538\n",
            "Epoch 221: | Loss: 0.32483 | Acc: 85.308\n",
            "Epoch 222: | Loss: 0.36378 | Acc: 82.154\n",
            "Epoch 223: | Loss: 0.36229 | Acc: 80.923\n",
            "Epoch 224: | Loss: 0.32549 | Acc: 82.846\n",
            "Epoch 225: | Loss: 0.35121 | Acc: 82.846\n",
            "Epoch 226: | Loss: 0.35524 | Acc: 82.538\n",
            "Epoch 227: | Loss: 0.33906 | Acc: 85.462\n",
            "Epoch 228: | Loss: 0.31432 | Acc: 83.462\n",
            "Epoch 229: | Loss: 0.32977 | Acc: 85.385\n",
            "Epoch 230: | Loss: 0.35809 | Acc: 83.769\n",
            "Epoch 231: | Loss: 0.32857 | Acc: 84.538\n",
            "Epoch 232: | Loss: 0.34613 | Acc: 82.692\n",
            "Epoch 233: | Loss: 0.31723 | Acc: 85.462\n",
            "Epoch 234: | Loss: 0.33945 | Acc: 83.231\n",
            "Epoch 235: | Loss: 0.32495 | Acc: 85.077\n",
            "Epoch 236: | Loss: 0.33067 | Acc: 84.231\n",
            "Epoch 237: | Loss: 0.31330 | Acc: 85.462\n",
            "Epoch 238: | Loss: 0.35576 | Acc: 85.462\n",
            "Epoch 239: | Loss: 0.32596 | Acc: 86.692\n",
            "Epoch 240: | Loss: 0.35261 | Acc: 84.077\n",
            "Epoch 241: | Loss: 0.31609 | Acc: 83.923\n",
            "Epoch 242: | Loss: 0.32245 | Acc: 85.615\n",
            "Epoch 243: | Loss: 0.34130 | Acc: 84.538\n",
            "Epoch 244: | Loss: 0.30949 | Acc: 86.385\n",
            "Epoch 245: | Loss: 0.33710 | Acc: 84.231\n",
            "Epoch 246: | Loss: 0.31944 | Acc: 85.308\n",
            "Epoch 247: | Loss: 0.30221 | Acc: 85.231\n",
            "Epoch 248: | Loss: 0.33514 | Acc: 85.308\n",
            "Epoch 249: | Loss: 0.32552 | Acc: 86.000\n",
            "Epoch 250: | Loss: 0.33448 | Acc: 84.385\n",
            "Epoch 251: | Loss: 0.33128 | Acc: 84.385\n",
            "Epoch 252: | Loss: 0.32133 | Acc: 86.000\n",
            "Epoch 253: | Loss: 0.32038 | Acc: 84.538\n",
            "Epoch 254: | Loss: 0.30344 | Acc: 86.385\n",
            "Epoch 255: | Loss: 0.33644 | Acc: 83.923\n",
            "Epoch 256: | Loss: 0.29814 | Acc: 86.462\n",
            "Epoch 257: | Loss: 0.31031 | Acc: 85.000\n",
            "Epoch 258: | Loss: 0.29719 | Acc: 86.077\n",
            "Epoch 259: | Loss: 0.30468 | Acc: 85.615\n",
            "Epoch 260: | Loss: 0.37953 | Acc: 82.462\n",
            "Epoch 261: | Loss: 0.30682 | Acc: 86.231\n",
            "Epoch 262: | Loss: 0.29720 | Acc: 87.462\n",
            "Epoch 263: | Loss: 0.29438 | Acc: 85.385\n",
            "Epoch 264: | Loss: 0.31969 | Acc: 84.538\n",
            "Epoch 265: | Loss: 0.32494 | Acc: 84.154\n",
            "Epoch 266: | Loss: 0.31827 | Acc: 86.385\n",
            "Epoch 267: | Loss: 0.29474 | Acc: 87.923\n",
            "Epoch 268: | Loss: 0.31273 | Acc: 84.231\n",
            "Epoch 269: | Loss: 0.29329 | Acc: 86.769\n",
            "Epoch 270: | Loss: 0.29013 | Acc: 88.154\n",
            "Epoch 271: | Loss: 0.29031 | Acc: 85.769\n",
            "Epoch 272: | Loss: 0.28928 | Acc: 87.385\n",
            "Epoch 273: | Loss: 0.29893 | Acc: 86.308\n",
            "Epoch 274: | Loss: 0.32819 | Acc: 84.538\n",
            "Epoch 275: | Loss: 0.31641 | Acc: 86.385\n",
            "Epoch 276: | Loss: 0.30229 | Acc: 87.154\n",
            "Epoch 277: | Loss: 0.32068 | Acc: 84.846\n",
            "Epoch 278: | Loss: 0.29876 | Acc: 86.154\n",
            "Epoch 279: | Loss: 0.31768 | Acc: 86.846\n",
            "Epoch 280: | Loss: 0.29217 | Acc: 86.615\n",
            "Epoch 281: | Loss: 0.29617 | Acc: 85.769\n",
            "Epoch 282: | Loss: 0.30711 | Acc: 84.692\n",
            "Epoch 283: | Loss: 0.27961 | Acc: 87.846\n",
            "Epoch 284: | Loss: 0.32967 | Acc: 83.769\n",
            "Epoch 285: | Loss: 0.27935 | Acc: 88.769\n",
            "Epoch 286: | Loss: 0.30214 | Acc: 85.923\n",
            "Epoch 287: | Loss: 0.33105 | Acc: 85.308\n",
            "Epoch 288: | Loss: 0.35754 | Acc: 83.000\n",
            "Epoch 289: | Loss: 0.31478 | Acc: 84.385\n",
            "Epoch 290: | Loss: 0.33813 | Acc: 84.615\n",
            "Epoch 291: | Loss: 0.35838 | Acc: 83.538\n",
            "Epoch 292: | Loss: 0.34004 | Acc: 85.231\n",
            "Epoch 293: | Loss: 0.31168 | Acc: 83.154\n",
            "Epoch 294: | Loss: 0.30264 | Acc: 86.154\n",
            "Epoch 295: | Loss: 0.31909 | Acc: 83.769\n",
            "Epoch 296: | Loss: 0.34502 | Acc: 85.615\n",
            "Epoch 297: | Loss: 0.27874 | Acc: 88.231\n",
            "Epoch 298: | Loss: 0.28485 | Acc: 87.385\n",
            "Epoch 299: | Loss: 0.29028 | Acc: 88.462\n",
            "Epoch 300: | Loss: 0.32306 | Acc: 86.000\n",
            "Epoch 301: | Loss: 0.31007 | Acc: 86.923\n",
            "Epoch 302: | Loss: 0.29843 | Acc: 84.077\n",
            "Epoch 303: | Loss: 0.32753 | Acc: 83.462\n",
            "Epoch 304: | Loss: 0.31317 | Acc: 86.462\n",
            "Epoch 305: | Loss: 0.32437 | Acc: 83.615\n",
            "Epoch 306: | Loss: 0.31021 | Acc: 86.077\n",
            "Epoch 307: | Loss: 0.28680 | Acc: 87.615\n",
            "Epoch 308: | Loss: 0.35206 | Acc: 84.077\n",
            "Epoch 309: | Loss: 0.29500 | Acc: 86.077\n",
            "Epoch 310: | Loss: 0.31429 | Acc: 85.923\n",
            "Epoch 311: | Loss: 0.29182 | Acc: 86.692\n",
            "Epoch 312: | Loss: 0.27737 | Acc: 86.923\n",
            "Epoch 313: | Loss: 0.32957 | Acc: 83.538\n",
            "Epoch 314: | Loss: 0.32914 | Acc: 84.769\n",
            "Epoch 315: | Loss: 0.32221 | Acc: 83.077\n",
            "Epoch 316: | Loss: 0.32164 | Acc: 85.692\n",
            "Epoch 317: | Loss: 0.31003 | Acc: 86.692\n",
            "Epoch 318: | Loss: 0.33964 | Acc: 84.769\n",
            "Epoch 319: | Loss: 0.31854 | Acc: 85.462\n",
            "Epoch 320: | Loss: 0.28716 | Acc: 85.846\n",
            "Epoch 321: | Loss: 0.30324 | Acc: 86.692\n",
            "Epoch 322: | Loss: 0.29650 | Acc: 86.769\n",
            "Epoch 323: | Loss: 0.27605 | Acc: 87.769\n",
            "Epoch 324: | Loss: 0.26660 | Acc: 87.538\n",
            "Epoch 325: | Loss: 0.30842 | Acc: 84.462\n",
            "Epoch 326: | Loss: 0.29315 | Acc: 87.000\n",
            "Epoch 327: | Loss: 0.29469 | Acc: 87.000\n",
            "Epoch 328: | Loss: 0.32111 | Acc: 84.538\n",
            "Epoch 329: | Loss: 0.34060 | Acc: 83.462\n",
            "Epoch 330: | Loss: 0.29738 | Acc: 86.385\n",
            "Epoch 331: | Loss: 0.29867 | Acc: 85.769\n",
            "Epoch 332: | Loss: 0.29358 | Acc: 85.462\n",
            "Epoch 333: | Loss: 0.29100 | Acc: 88.000\n",
            "Epoch 334: | Loss: 0.27310 | Acc: 87.923\n",
            "Epoch 335: | Loss: 0.30220 | Acc: 85.923\n",
            "Epoch 336: | Loss: 0.29837 | Acc: 85.615\n",
            "Epoch 337: | Loss: 0.32001 | Acc: 84.077\n",
            "Epoch 338: | Loss: 0.34466 | Acc: 83.923\n",
            "Epoch 339: | Loss: 0.31346 | Acc: 84.385\n",
            "Epoch 340: | Loss: 0.32669 | Acc: 85.077\n",
            "Epoch 341: | Loss: 0.33048 | Acc: 83.385\n",
            "Epoch 342: | Loss: 0.30544 | Acc: 86.538\n",
            "Epoch 343: | Loss: 0.29176 | Acc: 85.923\n",
            "Epoch 344: | Loss: 0.30523 | Acc: 85.769\n",
            "Epoch 345: | Loss: 0.28328 | Acc: 86.538\n",
            "Epoch 346: | Loss: 0.28865 | Acc: 87.923\n",
            "Epoch 347: | Loss: 0.30660 | Acc: 85.769\n",
            "Epoch 348: | Loss: 0.30788 | Acc: 85.923\n",
            "Epoch 349: | Loss: 0.30316 | Acc: 86.077\n",
            "Epoch 350: | Loss: 0.26971 | Acc: 88.385\n",
            "Epoch 351: | Loss: 0.29148 | Acc: 86.000\n",
            "Epoch 352: | Loss: 0.29655 | Acc: 87.154\n",
            "Epoch 353: | Loss: 0.34891 | Acc: 85.231\n",
            "Epoch 354: | Loss: 0.29949 | Acc: 87.000\n",
            "Epoch 355: | Loss: 0.30258 | Acc: 84.923\n",
            "Epoch 356: | Loss: 0.34453 | Acc: 84.154\n",
            "Epoch 357: | Loss: 0.28121 | Acc: 87.538\n",
            "Epoch 358: | Loss: 0.27564 | Acc: 87.154\n",
            "Epoch 359: | Loss: 0.28172 | Acc: 88.462\n",
            "Epoch 360: | Loss: 0.26934 | Acc: 87.077\n",
            "Epoch 361: | Loss: 0.26391 | Acc: 89.462\n",
            "Epoch 362: | Loss: 0.24959 | Acc: 88.846\n",
            "Epoch 363: | Loss: 0.26532 | Acc: 87.308\n",
            "Epoch 364: | Loss: 0.28336 | Acc: 87.462\n",
            "Epoch 365: | Loss: 0.29471 | Acc: 85.462\n",
            "Epoch 366: | Loss: 0.25302 | Acc: 88.308\n",
            "Epoch 367: | Loss: 0.28688 | Acc: 86.615\n",
            "Epoch 368: | Loss: 0.25704 | Acc: 90.154\n",
            "Epoch 369: | Loss: 0.27036 | Acc: 87.615\n",
            "Epoch 370: | Loss: 0.32883 | Acc: 84.769\n",
            "Epoch 371: | Loss: 0.26835 | Acc: 88.923\n",
            "Epoch 372: | Loss: 0.29269 | Acc: 87.308\n",
            "Epoch 373: | Loss: 0.26961 | Acc: 87.615\n",
            "Epoch 374: | Loss: 0.29127 | Acc: 87.692\n",
            "Epoch 375: | Loss: 0.30641 | Acc: 84.308\n",
            "Epoch 376: | Loss: 0.27070 | Acc: 88.308\n",
            "Epoch 377: | Loss: 0.26786 | Acc: 90.385\n",
            "Epoch 378: | Loss: 0.27633 | Acc: 88.923\n",
            "Epoch 379: | Loss: 0.29612 | Acc: 88.231\n",
            "Epoch 380: | Loss: 0.29320 | Acc: 86.308\n",
            "Epoch 381: | Loss: 0.30248 | Acc: 87.154\n",
            "Epoch 382: | Loss: 0.26428 | Acc: 87.769\n",
            "Epoch 383: | Loss: 0.32729 | Acc: 85.077\n",
            "Epoch 384: | Loss: 0.32536 | Acc: 83.692\n",
            "Epoch 385: | Loss: 0.30160 | Acc: 85.692\n",
            "Epoch 386: | Loss: 0.30105 | Acc: 85.231\n",
            "Epoch 387: | Loss: 0.28950 | Acc: 86.231\n",
            "Epoch 388: | Loss: 0.28727 | Acc: 86.462\n",
            "Epoch 389: | Loss: 0.23413 | Acc: 88.923\n",
            "Epoch 390: | Loss: 0.27357 | Acc: 87.308\n",
            "Epoch 391: | Loss: 0.26911 | Acc: 88.692\n",
            "Epoch 392: | Loss: 0.24911 | Acc: 88.231\n",
            "Epoch 393: | Loss: 0.32421 | Acc: 85.538\n",
            "Epoch 394: | Loss: 0.23511 | Acc: 89.538\n",
            "Epoch 395: | Loss: 0.24018 | Acc: 90.231\n",
            "Epoch 396: | Loss: 0.25903 | Acc: 88.385\n",
            "Epoch 397: | Loss: 0.25908 | Acc: 88.000\n",
            "Epoch 398: | Loss: 0.25024 | Acc: 89.154\n",
            "Epoch 399: | Loss: 0.22862 | Acc: 90.923\n",
            "Epoch 400: | Loss: 0.29472 | Acc: 86.615\n",
            "Epoch 401: | Loss: 0.29996 | Acc: 86.692\n",
            "Epoch 402: | Loss: 0.28573 | Acc: 86.000\n",
            "Epoch 403: | Loss: 0.28276 | Acc: 87.077\n",
            "Epoch 404: | Loss: 0.26976 | Acc: 88.462\n",
            "Epoch 405: | Loss: 0.24260 | Acc: 89.000\n",
            "Epoch 406: | Loss: 0.25329 | Acc: 88.846\n",
            "Epoch 407: | Loss: 0.27932 | Acc: 89.000\n",
            "Epoch 408: | Loss: 0.27524 | Acc: 87.462\n",
            "Epoch 409: | Loss: 0.27918 | Acc: 86.692\n",
            "Epoch 410: | Loss: 0.29386 | Acc: 86.769\n",
            "Epoch 411: | Loss: 0.27333 | Acc: 88.077\n",
            "Epoch 412: | Loss: 0.25588 | Acc: 89.462\n",
            "Epoch 413: | Loss: 0.30730 | Acc: 85.154\n",
            "Epoch 414: | Loss: 0.27907 | Acc: 87.000\n",
            "Epoch 415: | Loss: 0.26978 | Acc: 88.615\n",
            "Epoch 416: | Loss: 0.24777 | Acc: 89.462\n",
            "Epoch 417: | Loss: 0.28512 | Acc: 86.615\n",
            "Epoch 418: | Loss: 0.32573 | Acc: 83.462\n",
            "Epoch 419: | Loss: 0.26471 | Acc: 87.538\n",
            "Epoch 420: | Loss: 0.25925 | Acc: 88.231\n",
            "Epoch 421: | Loss: 0.27648 | Acc: 86.923\n",
            "Epoch 422: | Loss: 0.25431 | Acc: 89.231\n",
            "Epoch 423: | Loss: 0.25752 | Acc: 89.615\n",
            "Epoch 424: | Loss: 0.29015 | Acc: 87.231\n",
            "Epoch 425: | Loss: 0.28831 | Acc: 86.769\n",
            "Epoch 426: | Loss: 0.25494 | Acc: 87.615\n",
            "Epoch 427: | Loss: 0.24742 | Acc: 88.462\n",
            "Epoch 428: | Loss: 0.26003 | Acc: 89.077\n",
            "Epoch 429: | Loss: 0.33805 | Acc: 85.923\n",
            "Epoch 430: | Loss: 0.31803 | Acc: 85.923\n",
            "Epoch 431: | Loss: 0.28680 | Acc: 87.308\n",
            "Epoch 432: | Loss: 0.27912 | Acc: 89.385\n",
            "Epoch 433: | Loss: 0.26663 | Acc: 87.154\n",
            "Epoch 434: | Loss: 0.26107 | Acc: 88.692\n",
            "Epoch 435: | Loss: 0.37754 | Acc: 84.615\n",
            "Epoch 436: | Loss: 0.25422 | Acc: 89.308\n",
            "Epoch 437: | Loss: 0.25959 | Acc: 89.077\n",
            "Epoch 438: | Loss: 0.26426 | Acc: 87.846\n",
            "Epoch 439: | Loss: 0.30368 | Acc: 85.538\n",
            "Epoch 440: | Loss: 0.31439 | Acc: 86.000\n",
            "Epoch 441: | Loss: 0.29623 | Acc: 85.154\n",
            "Epoch 442: | Loss: 0.29927 | Acc: 85.308\n",
            "Epoch 443: | Loss: 0.30717 | Acc: 86.846\n",
            "Epoch 444: | Loss: 0.33431 | Acc: 83.692\n",
            "Epoch 445: | Loss: 0.34351 | Acc: 84.077\n",
            "Epoch 446: | Loss: 0.30807 | Acc: 85.154\n",
            "Epoch 447: | Loss: 0.29561 | Acc: 87.769\n",
            "Epoch 448: | Loss: 0.30966 | Acc: 86.231\n",
            "Epoch 449: | Loss: 0.29530 | Acc: 86.231\n",
            "Epoch 450: | Loss: 0.29792 | Acc: 86.538\n",
            "Epoch 451: | Loss: 0.28901 | Acc: 86.769\n",
            "Epoch 452: | Loss: 0.29926 | Acc: 86.154\n",
            "Epoch 453: | Loss: 0.28535 | Acc: 88.077\n",
            "Epoch 454: | Loss: 0.27867 | Acc: 88.231\n",
            "Epoch 455: | Loss: 0.33744 | Acc: 84.769\n",
            "Epoch 456: | Loss: 0.29905 | Acc: 87.308\n",
            "Epoch 457: | Loss: 0.26817 | Acc: 88.077\n",
            "Epoch 458: | Loss: 0.28331 | Acc: 86.231\n",
            "Epoch 459: | Loss: 0.30722 | Acc: 87.077\n",
            "Epoch 460: | Loss: 0.31529 | Acc: 84.000\n",
            "Epoch 461: | Loss: 0.25763 | Acc: 89.462\n",
            "Epoch 462: | Loss: 0.29338 | Acc: 85.615\n",
            "Epoch 463: | Loss: 0.29088 | Acc: 85.846\n",
            "Epoch 464: | Loss: 0.27869 | Acc: 85.769\n",
            "Epoch 465: | Loss: 0.31372 | Acc: 83.923\n",
            "Epoch 466: | Loss: 0.32121 | Acc: 86.769\n",
            "Epoch 467: | Loss: 0.35386 | Acc: 85.462\n",
            "Epoch 468: | Loss: 0.30344 | Acc: 87.462\n",
            "Epoch 469: | Loss: 0.31697 | Acc: 86.154\n",
            "Epoch 470: | Loss: 0.27444 | Acc: 87.538\n",
            "Epoch 471: | Loss: 0.27207 | Acc: 88.231\n",
            "Epoch 472: | Loss: 0.29850 | Acc: 85.615\n",
            "Epoch 473: | Loss: 0.29159 | Acc: 86.462\n",
            "Epoch 474: | Loss: 0.29852 | Acc: 87.385\n",
            "Epoch 475: | Loss: 0.29925 | Acc: 86.769\n",
            "Epoch 476: | Loss: 0.28353 | Acc: 87.462\n",
            "Epoch 477: | Loss: 0.30186 | Acc: 85.615\n",
            "Epoch 478: | Loss: 0.32680 | Acc: 84.923\n",
            "Epoch 479: | Loss: 0.28885 | Acc: 85.231\n",
            "Epoch 480: | Loss: 0.30673 | Acc: 87.769\n",
            "Epoch 481: | Loss: 0.31310 | Acc: 85.308\n",
            "Epoch 482: | Loss: 0.28283 | Acc: 87.692\n",
            "Epoch 483: | Loss: 0.27653 | Acc: 89.000\n",
            "Epoch 484: | Loss: 0.28595 | Acc: 86.077\n",
            "Epoch 485: | Loss: 0.27488 | Acc: 87.462\n",
            "Epoch 486: | Loss: 0.26693 | Acc: 88.769\n",
            "Epoch 487: | Loss: 0.27876 | Acc: 88.923\n",
            "Epoch 488: | Loss: 0.28239 | Acc: 88.615\n",
            "Epoch 489: | Loss: 0.25932 | Acc: 88.154\n",
            "Epoch 490: | Loss: 0.27911 | Acc: 87.154\n",
            "Epoch 491: | Loss: 0.29517 | Acc: 88.077\n",
            "Epoch 492: | Loss: 0.25096 | Acc: 89.692\n",
            "Epoch 493: | Loss: 0.25733 | Acc: 89.000\n",
            "Epoch 494: | Loss: 0.24995 | Acc: 89.000\n",
            "Epoch 495: | Loss: 0.27460 | Acc: 86.769\n",
            "Epoch 496: | Loss: 0.28559 | Acc: 86.615\n",
            "Epoch 497: | Loss: 0.26340 | Acc: 87.385\n",
            "Epoch 498: | Loss: 0.29279 | Acc: 85.615\n",
            "Epoch 499: | Loss: 0.28072 | Acc: 88.000\n",
            "Epoch 500: | Loss: 0.27084 | Acc: 87.769\n",
            "Epoch 501: | Loss: 0.23804 | Acc: 90.154\n",
            "Epoch 502: | Loss: 0.33333 | Acc: 84.923\n",
            "Epoch 503: | Loss: 0.26492 | Acc: 87.000\n",
            "Epoch 504: | Loss: 0.25906 | Acc: 89.077\n",
            "Epoch 505: | Loss: 0.25564 | Acc: 87.923\n",
            "Epoch 506: | Loss: 0.26841 | Acc: 85.231\n",
            "Epoch 507: | Loss: 0.25271 | Acc: 87.308\n",
            "Epoch 508: | Loss: 0.30262 | Acc: 88.308\n",
            "Epoch 509: | Loss: 0.23361 | Acc: 90.000\n",
            "Epoch 510: | Loss: 0.28509 | Acc: 86.615\n",
            "Epoch 511: | Loss: 0.25673 | Acc: 87.769\n",
            "Epoch 512: | Loss: 0.25584 | Acc: 87.308\n",
            "Epoch 513: | Loss: 0.25133 | Acc: 88.846\n",
            "Epoch 514: | Loss: 0.29581 | Acc: 86.538\n",
            "Epoch 515: | Loss: 0.25340 | Acc: 88.538\n",
            "Epoch 516: | Loss: 0.27091 | Acc: 87.923\n",
            "Epoch 517: | Loss: 0.23634 | Acc: 91.154\n",
            "Epoch 518: | Loss: 0.31720 | Acc: 85.769\n",
            "Epoch 519: | Loss: 0.27532 | Acc: 87.923\n",
            "Epoch 520: | Loss: 0.32086 | Acc: 84.846\n",
            "Epoch 521: | Loss: 0.29495 | Acc: 86.231\n",
            "Epoch 522: | Loss: 0.29895 | Acc: 85.308\n",
            "Epoch 523: | Loss: 0.24454 | Acc: 89.615\n",
            "Epoch 524: | Loss: 0.37666 | Acc: 86.692\n",
            "Epoch 525: | Loss: 0.25198 | Acc: 88.769\n",
            "Epoch 526: | Loss: 0.25839 | Acc: 90.154\n",
            "Epoch 527: | Loss: 0.27155 | Acc: 89.692\n",
            "Epoch 528: | Loss: 0.25261 | Acc: 89.154\n",
            "Epoch 529: | Loss: 0.27394 | Acc: 89.231\n",
            "Epoch 530: | Loss: 0.23212 | Acc: 88.692\n",
            "Epoch 531: | Loss: 0.25018 | Acc: 87.923\n",
            "Epoch 532: | Loss: 0.26082 | Acc: 88.692\n",
            "Epoch 533: | Loss: 0.26541 | Acc: 87.615\n",
            "Epoch 534: | Loss: 0.25644 | Acc: 88.846\n",
            "Epoch 535: | Loss: 0.27019 | Acc: 88.385\n",
            "Epoch 536: | Loss: 0.28283 | Acc: 87.923\n",
            "Epoch 537: | Loss: 0.26466 | Acc: 87.846\n",
            "Epoch 538: | Loss: 0.26419 | Acc: 88.154\n",
            "Epoch 539: | Loss: 0.29774 | Acc: 86.846\n",
            "Epoch 540: | Loss: 0.27598 | Acc: 87.077\n",
            "Epoch 541: | Loss: 0.34598 | Acc: 85.538\n",
            "Epoch 542: | Loss: 0.27035 | Acc: 88.308\n",
            "Epoch 543: | Loss: 0.26294 | Acc: 88.231\n",
            "Epoch 544: | Loss: 0.27435 | Acc: 89.000\n",
            "Epoch 545: | Loss: 0.24144 | Acc: 89.154\n",
            "Epoch 546: | Loss: 0.27851 | Acc: 86.077\n",
            "Epoch 547: | Loss: 0.23259 | Acc: 90.000\n",
            "Epoch 548: | Loss: 0.30499 | Acc: 85.769\n",
            "Epoch 549: | Loss: 0.26630 | Acc: 87.000\n",
            "Epoch 550: | Loss: 0.31631 | Acc: 84.846\n",
            "Epoch 551: | Loss: 0.24111 | Acc: 88.615\n",
            "Epoch 552: | Loss: 0.23456 | Acc: 90.846\n",
            "Epoch 553: | Loss: 0.23725 | Acc: 90.231\n",
            "Epoch 554: | Loss: 0.24704 | Acc: 89.462\n",
            "Epoch 555: | Loss: 0.24186 | Acc: 90.154\n",
            "Epoch 556: | Loss: 0.27695 | Acc: 88.846\n",
            "Epoch 557: | Loss: 0.25016 | Acc: 88.077\n",
            "Epoch 558: | Loss: 0.22890 | Acc: 90.154\n",
            "Epoch 559: | Loss: 0.25829 | Acc: 88.000\n",
            "Epoch 560: | Loss: 0.25523 | Acc: 88.308\n",
            "Epoch 561: | Loss: 0.25302 | Acc: 89.615\n",
            "Epoch 562: | Loss: 0.25541 | Acc: 88.385\n",
            "Epoch 563: | Loss: 0.29585 | Acc: 87.308\n",
            "Epoch 564: | Loss: 0.28338 | Acc: 87.308\n",
            "Epoch 565: | Loss: 0.25138 | Acc: 86.923\n",
            "Epoch 566: | Loss: 0.28125 | Acc: 86.462\n",
            "Epoch 567: | Loss: 0.25230 | Acc: 88.154\n",
            "Epoch 568: | Loss: 0.24083 | Acc: 89.385\n",
            "Epoch 569: | Loss: 0.23442 | Acc: 91.077\n",
            "Epoch 570: | Loss: 0.24707 | Acc: 89.231\n",
            "Epoch 571: | Loss: 0.22247 | Acc: 89.385\n",
            "Epoch 572: | Loss: 0.26355 | Acc: 89.231\n",
            "Epoch 573: | Loss: 0.29097 | Acc: 88.154\n",
            "Epoch 574: | Loss: 0.25041 | Acc: 90.231\n",
            "Epoch 575: | Loss: 0.29963 | Acc: 86.077\n",
            "Epoch 576: | Loss: 0.27354 | Acc: 86.538\n",
            "Epoch 577: | Loss: 0.28672 | Acc: 86.846\n",
            "Epoch 578: | Loss: 0.22643 | Acc: 89.923\n",
            "Epoch 579: | Loss: 0.27500 | Acc: 88.231\n",
            "Epoch 580: | Loss: 0.22934 | Acc: 90.923\n",
            "Epoch 581: | Loss: 0.27671 | Acc: 85.462\n",
            "Epoch 582: | Loss: 0.24346 | Acc: 88.692\n",
            "Epoch 583: | Loss: 0.25358 | Acc: 89.154\n",
            "Epoch 584: | Loss: 0.23259 | Acc: 91.077\n",
            "Epoch 585: | Loss: 0.24198 | Acc: 89.923\n",
            "Epoch 586: | Loss: 0.22962 | Acc: 90.077\n",
            "Epoch 587: | Loss: 0.21398 | Acc: 91.231\n",
            "Epoch 588: | Loss: 0.22160 | Acc: 90.385\n",
            "Epoch 589: | Loss: 0.25311 | Acc: 89.154\n",
            "Epoch 590: | Loss: 0.22660 | Acc: 89.692\n",
            "Epoch 591: | Loss: 0.22737 | Acc: 88.692\n",
            "Epoch 592: | Loss: 0.22577 | Acc: 90.308\n",
            "Epoch 593: | Loss: 0.19160 | Acc: 92.462\n",
            "Epoch 594: | Loss: 0.21603 | Acc: 92.308\n",
            "Epoch 595: | Loss: 0.22647 | Acc: 90.154\n",
            "Epoch 596: | Loss: 0.24304 | Acc: 89.000\n",
            "Epoch 597: | Loss: 0.22036 | Acc: 90.308\n",
            "Epoch 598: | Loss: 0.23308 | Acc: 89.923\n",
            "Epoch 599: | Loss: 0.21233 | Acc: 91.538\n",
            "Epoch 600: | Loss: 0.30237 | Acc: 87.154\n",
            "Epoch 601: | Loss: 0.23386 | Acc: 90.846\n",
            "Epoch 602: | Loss: 0.24194 | Acc: 90.385\n",
            "Epoch 603: | Loss: 0.21566 | Acc: 91.000\n",
            "Epoch 604: | Loss: 0.28805 | Acc: 86.538\n",
            "Epoch 605: | Loss: 0.25548 | Acc: 88.846\n",
            "Epoch 606: | Loss: 0.20398 | Acc: 90.154\n",
            "Epoch 607: | Loss: 0.19434 | Acc: 92.385\n",
            "Epoch 608: | Loss: 0.23866 | Acc: 88.615\n",
            "Epoch 609: | Loss: 0.24786 | Acc: 88.385\n",
            "Epoch 610: | Loss: 0.24843 | Acc: 88.385\n",
            "Epoch 611: | Loss: 0.24271 | Acc: 90.385\n",
            "Epoch 612: | Loss: 0.24385 | Acc: 89.769\n",
            "Epoch 613: | Loss: 0.22857 | Acc: 89.385\n",
            "Epoch 614: | Loss: 0.24574 | Acc: 88.923\n",
            "Epoch 615: | Loss: 0.23670 | Acc: 88.385\n",
            "Epoch 616: | Loss: 0.23403 | Acc: 90.385\n",
            "Epoch 617: | Loss: 0.25314 | Acc: 87.692\n",
            "Epoch 618: | Loss: 0.22602 | Acc: 90.538\n",
            "Epoch 619: | Loss: 0.23712 | Acc: 88.692\n",
            "Epoch 620: | Loss: 0.23098 | Acc: 89.846\n",
            "Epoch 621: | Loss: 0.26203 | Acc: 89.308\n",
            "Epoch 622: | Loss: 0.24261 | Acc: 89.231\n",
            "Epoch 623: | Loss: 0.31046 | Acc: 87.923\n",
            "Epoch 624: | Loss: 0.22755 | Acc: 91.692\n",
            "Epoch 625: | Loss: 0.25577 | Acc: 88.462\n",
            "Epoch 626: | Loss: 0.24017 | Acc: 88.846\n",
            "Epoch 627: | Loss: 0.21951 | Acc: 90.385\n",
            "Epoch 628: | Loss: 0.20922 | Acc: 91.231\n",
            "Epoch 629: | Loss: 0.20775 | Acc: 90.615\n",
            "Epoch 630: | Loss: 0.27034 | Acc: 88.308\n",
            "Epoch 631: | Loss: 0.23424 | Acc: 89.308\n",
            "Epoch 632: | Loss: 0.21947 | Acc: 91.000\n",
            "Epoch 633: | Loss: 0.22407 | Acc: 90.154\n",
            "Epoch 634: | Loss: 0.27050 | Acc: 89.923\n",
            "Epoch 635: | Loss: 0.26384 | Acc: 87.769\n",
            "Epoch 636: | Loss: 0.27320 | Acc: 87.846\n",
            "Epoch 637: | Loss: 0.24426 | Acc: 88.692\n",
            "Epoch 638: | Loss: 0.25752 | Acc: 91.000\n",
            "Epoch 639: | Loss: 0.26588 | Acc: 88.846\n",
            "Epoch 640: | Loss: 0.30187 | Acc: 87.154\n",
            "Epoch 641: | Loss: 0.25213 | Acc: 89.385\n",
            "Epoch 642: | Loss: 0.26859 | Acc: 87.846\n",
            "Epoch 643: | Loss: 0.23069 | Acc: 89.308\n",
            "Epoch 644: | Loss: 0.25000 | Acc: 90.462\n",
            "Epoch 645: | Loss: 0.21566 | Acc: 89.923\n",
            "Epoch 646: | Loss: 0.22464 | Acc: 89.308\n",
            "Epoch 647: | Loss: 0.24376 | Acc: 88.846\n",
            "Epoch 648: | Loss: 0.22266 | Acc: 90.923\n",
            "Epoch 649: | Loss: 0.22795 | Acc: 90.923\n",
            "Epoch 650: | Loss: 0.22572 | Acc: 90.231\n",
            "Epoch 651: | Loss: 0.28003 | Acc: 87.692\n",
            "Epoch 652: | Loss: 0.23678 | Acc: 89.692\n",
            "Epoch 653: | Loss: 0.24035 | Acc: 90.846\n",
            "Epoch 654: | Loss: 0.26554 | Acc: 87.000\n",
            "Epoch 655: | Loss: 0.34215 | Acc: 87.000\n",
            "Epoch 656: | Loss: 0.26524 | Acc: 87.308\n",
            "Epoch 657: | Loss: 0.26454 | Acc: 87.077\n",
            "Epoch 658: | Loss: 0.30254 | Acc: 86.385\n",
            "Epoch 659: | Loss: 0.29127 | Acc: 87.154\n",
            "Epoch 660: | Loss: 0.23114 | Acc: 90.385\n",
            "Epoch 661: | Loss: 0.28481 | Acc: 87.923\n",
            "Epoch 662: | Loss: 0.23344 | Acc: 90.846\n",
            "Epoch 663: | Loss: 0.25648 | Acc: 89.385\n",
            "Epoch 664: | Loss: 0.26368 | Acc: 86.308\n",
            "Epoch 665: | Loss: 0.28423 | Acc: 87.462\n",
            "Epoch 666: | Loss: 0.27403 | Acc: 87.000\n",
            "Epoch 667: | Loss: 0.25788 | Acc: 89.154\n",
            "Epoch 668: | Loss: 0.27050 | Acc: 87.000\n",
            "Epoch 669: | Loss: 0.25876 | Acc: 88.538\n",
            "Epoch 670: | Loss: 0.24412 | Acc: 89.385\n",
            "Epoch 671: | Loss: 0.23439 | Acc: 90.154\n",
            "Epoch 672: | Loss: 0.24626 | Acc: 88.846\n",
            "Epoch 673: | Loss: 0.25996 | Acc: 88.538\n",
            "Epoch 674: | Loss: 0.25717 | Acc: 89.077\n",
            "Epoch 675: | Loss: 0.23559 | Acc: 90.231\n",
            "Epoch 676: | Loss: 0.21836 | Acc: 90.538\n",
            "Epoch 677: | Loss: 0.20413 | Acc: 91.462\n",
            "Epoch 678: | Loss: 0.25414 | Acc: 90.000\n",
            "Epoch 679: | Loss: 0.19788 | Acc: 92.615\n",
            "Epoch 680: | Loss: 0.23531 | Acc: 88.769\n",
            "Epoch 681: | Loss: 0.22806 | Acc: 91.077\n",
            "Epoch 682: | Loss: 0.22969 | Acc: 89.308\n",
            "Epoch 683: | Loss: 0.22335 | Acc: 90.308\n",
            "Epoch 684: | Loss: 0.22389 | Acc: 90.000\n",
            "Epoch 685: | Loss: 0.23193 | Acc: 91.308\n",
            "Epoch 686: | Loss: 0.27335 | Acc: 85.615\n",
            "Epoch 687: | Loss: 0.24406 | Acc: 88.923\n",
            "Epoch 688: | Loss: 0.30180 | Acc: 87.923\n",
            "Epoch 689: | Loss: 0.24833 | Acc: 89.462\n",
            "Epoch 690: | Loss: 0.32499 | Acc: 87.615\n",
            "Epoch 691: | Loss: 0.27035 | Acc: 86.769\n",
            "Epoch 692: | Loss: 0.23533 | Acc: 89.692\n",
            "Epoch 693: | Loss: 0.24821 | Acc: 89.077\n",
            "Epoch 694: | Loss: 0.26070 | Acc: 89.462\n",
            "Epoch 695: | Loss: 0.27172 | Acc: 88.385\n",
            "Epoch 696: | Loss: 0.24865 | Acc: 89.308\n",
            "Epoch 697: | Loss: 0.26391 | Acc: 87.769\n",
            "Epoch 698: | Loss: 0.29090 | Acc: 87.769\n",
            "Epoch 699: | Loss: 0.28815 | Acc: 87.615\n",
            "Epoch 700: | Loss: 0.30943 | Acc: 86.385\n",
            "Epoch 701: | Loss: 0.23534 | Acc: 89.538\n",
            "Epoch 702: | Loss: 0.23394 | Acc: 89.462\n",
            "Epoch 703: | Loss: 0.26349 | Acc: 88.000\n",
            "Epoch 704: | Loss: 0.25711 | Acc: 88.231\n",
            "Epoch 705: | Loss: 0.24701 | Acc: 89.615\n",
            "Epoch 706: | Loss: 0.24718 | Acc: 89.000\n",
            "Epoch 707: | Loss: 0.23861 | Acc: 90.077\n",
            "Epoch 708: | Loss: 0.26662 | Acc: 89.077\n",
            "Epoch 709: | Loss: 0.23766 | Acc: 89.538\n",
            "Epoch 710: | Loss: 0.23503 | Acc: 91.154\n",
            "Epoch 711: | Loss: 0.23788 | Acc: 90.000\n",
            "Epoch 712: | Loss: 0.24576 | Acc: 88.615\n",
            "Epoch 713: | Loss: 0.25126 | Acc: 88.308\n",
            "Epoch 714: | Loss: 0.23264 | Acc: 90.077\n",
            "Epoch 715: | Loss: 0.23564 | Acc: 90.308\n",
            "Epoch 716: | Loss: 0.22782 | Acc: 89.769\n",
            "Epoch 717: | Loss: 0.26016 | Acc: 89.154\n",
            "Epoch 718: | Loss: 0.26180 | Acc: 88.923\n",
            "Epoch 719: | Loss: 0.22637 | Acc: 89.538\n",
            "Epoch 720: | Loss: 0.23662 | Acc: 89.077\n",
            "Epoch 721: | Loss: 0.27484 | Acc: 86.846\n",
            "Epoch 722: | Loss: 0.26017 | Acc: 89.154\n",
            "Epoch 723: | Loss: 0.27603 | Acc: 89.000\n",
            "Epoch 724: | Loss: 0.25509 | Acc: 89.077\n",
            "Epoch 725: | Loss: 0.26235 | Acc: 88.077\n",
            "Epoch 726: | Loss: 0.24648 | Acc: 89.769\n",
            "Epoch 727: | Loss: 0.27056 | Acc: 92.231\n",
            "Epoch 728: | Loss: 0.22938 | Acc: 89.154\n",
            "Epoch 729: | Loss: 0.25849 | Acc: 89.154\n",
            "Epoch 730: | Loss: 0.25233 | Acc: 89.308\n",
            "Epoch 731: | Loss: 0.25173 | Acc: 89.077\n",
            "Epoch 732: | Loss: 0.30370 | Acc: 86.308\n",
            "Epoch 733: | Loss: 0.25583 | Acc: 90.231\n",
            "Epoch 734: | Loss: 0.29863 | Acc: 87.692\n",
            "Epoch 735: | Loss: 0.25381 | Acc: 88.846\n",
            "Epoch 736: | Loss: 0.25450 | Acc: 89.154\n",
            "Epoch 737: | Loss: 0.24217 | Acc: 89.769\n",
            "Epoch 738: | Loss: 0.22144 | Acc: 91.000\n",
            "Epoch 739: | Loss: 0.23845 | Acc: 89.077\n",
            "Epoch 740: | Loss: 0.22959 | Acc: 91.462\n",
            "Epoch 741: | Loss: 0.21143 | Acc: 90.308\n",
            "Epoch 742: | Loss: 0.18560 | Acc: 93.154\n",
            "Epoch 743: | Loss: 0.20864 | Acc: 91.846\n",
            "Epoch 744: | Loss: 0.20440 | Acc: 92.308\n",
            "Epoch 745: | Loss: 0.23608 | Acc: 89.615\n",
            "Epoch 746: | Loss: 0.21991 | Acc: 92.308\n",
            "Epoch 747: | Loss: 0.21495 | Acc: 91.923\n",
            "Epoch 748: | Loss: 0.22980 | Acc: 89.692\n",
            "Epoch 749: | Loss: 0.20722 | Acc: 90.923\n",
            "Epoch 750: | Loss: 0.21961 | Acc: 90.923\n",
            "Epoch 751: | Loss: 0.23065 | Acc: 91.077\n",
            "Epoch 752: | Loss: 0.22189 | Acc: 90.615\n",
            "Epoch 753: | Loss: 0.24653 | Acc: 88.769\n",
            "Epoch 754: | Loss: 0.18769 | Acc: 92.308\n",
            "Epoch 755: | Loss: 0.19316 | Acc: 90.615\n",
            "Epoch 756: | Loss: 0.22624 | Acc: 89.462\n",
            "Epoch 757: | Loss: 0.21000 | Acc: 90.385\n",
            "Epoch 758: | Loss: 0.20415 | Acc: 92.154\n",
            "Epoch 759: | Loss: 0.23218 | Acc: 90.000\n",
            "Epoch 760: | Loss: 0.21987 | Acc: 89.846\n",
            "Epoch 761: | Loss: 0.23118 | Acc: 90.077\n",
            "Epoch 762: | Loss: 0.23145 | Acc: 90.000\n",
            "Epoch 763: | Loss: 0.25029 | Acc: 89.000\n",
            "Epoch 764: | Loss: 0.26282 | Acc: 89.154\n",
            "Epoch 765: | Loss: 0.22578 | Acc: 89.462\n",
            "Epoch 766: | Loss: 0.21818 | Acc: 90.231\n",
            "Epoch 767: | Loss: 0.19366 | Acc: 92.462\n",
            "Epoch 768: | Loss: 0.23375 | Acc: 90.154\n",
            "Epoch 769: | Loss: 0.24801 | Acc: 86.154\n",
            "Epoch 770: | Loss: 0.20736 | Acc: 90.846\n",
            "Epoch 771: | Loss: 0.22632 | Acc: 90.923\n",
            "Epoch 772: | Loss: 0.21169 | Acc: 90.538\n",
            "Epoch 773: | Loss: 0.19112 | Acc: 92.308\n",
            "Epoch 774: | Loss: 0.23450 | Acc: 89.923\n",
            "Epoch 775: | Loss: 0.24293 | Acc: 89.923\n",
            "Epoch 776: | Loss: 0.30899 | Acc: 88.462\n",
            "Epoch 777: | Loss: 0.23037 | Acc: 89.923\n",
            "Epoch 778: | Loss: 0.17979 | Acc: 92.462\n",
            "Epoch 779: | Loss: 0.21416 | Acc: 90.538\n",
            "Epoch 780: | Loss: 0.27191 | Acc: 89.769\n",
            "Epoch 781: | Loss: 0.21916 | Acc: 89.769\n",
            "Epoch 782: | Loss: 0.19126 | Acc: 91.462\n",
            "Epoch 783: | Loss: 0.22040 | Acc: 89.615\n",
            "Epoch 784: | Loss: 0.20455 | Acc: 91.923\n",
            "Epoch 785: | Loss: 0.21634 | Acc: 91.154\n",
            "Epoch 786: | Loss: 0.23161 | Acc: 91.923\n",
            "Epoch 787: | Loss: 0.21000 | Acc: 90.308\n",
            "Epoch 788: | Loss: 0.21475 | Acc: 90.231\n",
            "Epoch 789: | Loss: 0.22929 | Acc: 89.615\n",
            "Epoch 790: | Loss: 0.18709 | Acc: 92.538\n",
            "Epoch 791: | Loss: 0.22480 | Acc: 90.077\n",
            "Epoch 792: | Loss: 0.21862 | Acc: 90.231\n",
            "Epoch 793: | Loss: 0.19671 | Acc: 91.769\n",
            "Epoch 794: | Loss: 0.20614 | Acc: 91.923\n",
            "Epoch 795: | Loss: 0.23161 | Acc: 89.846\n",
            "Epoch 796: | Loss: 0.25155 | Acc: 90.385\n",
            "Epoch 797: | Loss: 0.22126 | Acc: 88.538\n",
            "Epoch 798: | Loss: 0.24402 | Acc: 90.538\n",
            "Epoch 799: | Loss: 0.20455 | Acc: 90.000\n",
            "Epoch 800: | Loss: 0.19477 | Acc: 92.308\n",
            "Epoch 801: | Loss: 0.20381 | Acc: 91.385\n",
            "Epoch 802: | Loss: 0.21666 | Acc: 90.846\n",
            "Epoch 803: | Loss: 0.27199 | Acc: 90.769\n",
            "Epoch 804: | Loss: 0.21048 | Acc: 89.923\n",
            "Epoch 805: | Loss: 0.22857 | Acc: 90.231\n",
            "Epoch 806: | Loss: 0.21397 | Acc: 91.462\n",
            "Epoch 807: | Loss: 0.19846 | Acc: 91.538\n",
            "Epoch 808: | Loss: 0.21437 | Acc: 90.308\n",
            "Epoch 809: | Loss: 0.24337 | Acc: 89.846\n",
            "Epoch 810: | Loss: 0.18475 | Acc: 92.846\n",
            "Epoch 811: | Loss: 0.17454 | Acc: 93.538\n",
            "Epoch 812: | Loss: 0.23039 | Acc: 91.308\n",
            "Epoch 813: | Loss: 0.22369 | Acc: 90.385\n",
            "Epoch 814: | Loss: 0.22684 | Acc: 90.308\n",
            "Epoch 815: | Loss: 0.20431 | Acc: 91.231\n",
            "Epoch 816: | Loss: 0.19256 | Acc: 92.308\n",
            "Epoch 817: | Loss: 0.25971 | Acc: 89.077\n",
            "Epoch 818: | Loss: 0.23064 | Acc: 90.538\n",
            "Epoch 819: | Loss: 0.28165 | Acc: 86.538\n",
            "Epoch 820: | Loss: 0.20316 | Acc: 90.692\n",
            "Epoch 821: | Loss: 0.16974 | Acc: 93.538\n",
            "Epoch 822: | Loss: 0.26610 | Acc: 92.000\n",
            "Epoch 823: | Loss: 0.18152 | Acc: 92.385\n",
            "Epoch 824: | Loss: 0.21913 | Acc: 89.462\n",
            "Epoch 825: | Loss: 0.23877 | Acc: 91.462\n",
            "Epoch 826: | Loss: 0.18782 | Acc: 92.462\n",
            "Epoch 827: | Loss: 0.23909 | Acc: 90.385\n",
            "Epoch 828: | Loss: 0.19911 | Acc: 91.154\n",
            "Epoch 829: | Loss: 0.25456 | Acc: 88.846\n",
            "Epoch 830: | Loss: 0.17059 | Acc: 92.308\n",
            "Epoch 831: | Loss: 0.16809 | Acc: 92.923\n",
            "Epoch 832: | Loss: 0.24361 | Acc: 89.308\n",
            "Epoch 833: | Loss: 0.21565 | Acc: 91.154\n",
            "Epoch 834: | Loss: 0.24712 | Acc: 89.615\n",
            "Epoch 835: | Loss: 0.22911 | Acc: 90.769\n",
            "Epoch 836: | Loss: 0.19392 | Acc: 91.154\n",
            "Epoch 837: | Loss: 0.22363 | Acc: 91.154\n",
            "Epoch 838: | Loss: 0.25210 | Acc: 89.615\n",
            "Epoch 839: | Loss: 0.18471 | Acc: 93.462\n",
            "Epoch 840: | Loss: 0.29787 | Acc: 88.462\n",
            "Epoch 841: | Loss: 0.19058 | Acc: 92.462\n",
            "Epoch 842: | Loss: 0.20051 | Acc: 91.308\n",
            "Epoch 843: | Loss: 0.23141 | Acc: 90.538\n",
            "Epoch 844: | Loss: 0.22942 | Acc: 90.692\n",
            "Epoch 845: | Loss: 0.24675 | Acc: 91.000\n",
            "Epoch 846: | Loss: 0.19521 | Acc: 92.385\n",
            "Epoch 847: | Loss: 0.21653 | Acc: 91.923\n",
            "Epoch 848: | Loss: 0.28234 | Acc: 87.692\n",
            "Epoch 849: | Loss: 0.19771 | Acc: 92.154\n",
            "Epoch 850: | Loss: 0.25008 | Acc: 89.923\n",
            "Epoch 851: | Loss: 0.21509 | Acc: 91.231\n",
            "Epoch 852: | Loss: 0.27466 | Acc: 88.846\n",
            "Epoch 853: | Loss: 0.22402 | Acc: 90.385\n",
            "Epoch 854: | Loss: 0.20070 | Acc: 92.385\n",
            "Epoch 855: | Loss: 0.19499 | Acc: 92.308\n",
            "Epoch 856: | Loss: 0.17506 | Acc: 92.692\n",
            "Epoch 857: | Loss: 0.22668 | Acc: 90.154\n",
            "Epoch 858: | Loss: 0.21784 | Acc: 91.154\n",
            "Epoch 859: | Loss: 0.21063 | Acc: 90.692\n",
            "Epoch 860: | Loss: 0.19880 | Acc: 91.769\n",
            "Epoch 861: | Loss: 0.23874 | Acc: 89.769\n",
            "Epoch 862: | Loss: 0.20339 | Acc: 91.615\n",
            "Epoch 863: | Loss: 0.21692 | Acc: 89.154\n",
            "Epoch 864: | Loss: 0.22275 | Acc: 91.154\n",
            "Epoch 865: | Loss: 0.20687 | Acc: 91.154\n",
            "Epoch 866: | Loss: 0.20935 | Acc: 91.769\n",
            "Epoch 867: | Loss: 0.20245 | Acc: 92.077\n",
            "Epoch 868: | Loss: 0.21640 | Acc: 90.615\n",
            "Epoch 869: | Loss: 0.22619 | Acc: 89.538\n",
            "Epoch 870: | Loss: 0.20322 | Acc: 90.615\n",
            "Epoch 871: | Loss: 0.23555 | Acc: 90.538\n",
            "Epoch 872: | Loss: 0.22904 | Acc: 90.077\n",
            "Epoch 873: | Loss: 0.22740 | Acc: 89.154\n",
            "Epoch 874: | Loss: 0.20076 | Acc: 92.231\n",
            "Epoch 875: | Loss: 0.24786 | Acc: 90.385\n",
            "Epoch 876: | Loss: 0.21693 | Acc: 90.923\n",
            "Epoch 877: | Loss: 0.23388 | Acc: 90.308\n",
            "Epoch 878: | Loss: 0.22560 | Acc: 89.615\n",
            "Epoch 879: | Loss: 0.21620 | Acc: 91.154\n",
            "Epoch 880: | Loss: 0.18709 | Acc: 92.846\n",
            "Epoch 881: | Loss: 0.21550 | Acc: 90.846\n",
            "Epoch 882: | Loss: 0.17730 | Acc: 93.538\n",
            "Epoch 883: | Loss: 0.20005 | Acc: 91.308\n",
            "Epoch 884: | Loss: 0.21588 | Acc: 90.000\n",
            "Epoch 885: | Loss: 0.29002 | Acc: 89.000\n",
            "Epoch 886: | Loss: 0.24894 | Acc: 87.308\n",
            "Epoch 887: | Loss: 0.29259 | Acc: 87.154\n",
            "Epoch 888: | Loss: 0.26213 | Acc: 89.385\n",
            "Epoch 889: | Loss: 0.24033 | Acc: 89.077\n",
            "Epoch 890: | Loss: 0.24999 | Acc: 90.615\n",
            "Epoch 891: | Loss: 0.19909 | Acc: 91.077\n",
            "Epoch 892: | Loss: 0.27735 | Acc: 89.308\n",
            "Epoch 893: | Loss: 0.24502 | Acc: 90.308\n",
            "Epoch 894: | Loss: 0.21241 | Acc: 91.308\n",
            "Epoch 895: | Loss: 0.20292 | Acc: 91.231\n",
            "Epoch 896: | Loss: 0.27848 | Acc: 87.385\n",
            "Epoch 897: | Loss: 0.31870 | Acc: 87.462\n",
            "Epoch 898: | Loss: 0.24199 | Acc: 89.154\n",
            "Epoch 899: | Loss: 0.24279 | Acc: 89.923\n",
            "Epoch 900: | Loss: 0.27793 | Acc: 88.692\n",
            "Epoch 901: | Loss: 0.21428 | Acc: 91.231\n",
            "Epoch 902: | Loss: 0.20970 | Acc: 92.308\n",
            "Epoch 903: | Loss: 0.24042 | Acc: 91.385\n",
            "Epoch 904: | Loss: 0.23767 | Acc: 90.231\n",
            "Epoch 905: | Loss: 0.24389 | Acc: 90.846\n",
            "Epoch 906: | Loss: 0.28697 | Acc: 88.077\n",
            "Epoch 907: | Loss: 0.21568 | Acc: 91.615\n",
            "Epoch 908: | Loss: 0.22207 | Acc: 89.692\n",
            "Epoch 909: | Loss: 0.19052 | Acc: 91.231\n",
            "Epoch 910: | Loss: 0.20980 | Acc: 90.692\n",
            "Epoch 911: | Loss: 0.18601 | Acc: 92.615\n",
            "Epoch 912: | Loss: 0.19328 | Acc: 91.000\n",
            "Epoch 913: | Loss: 0.22349 | Acc: 91.308\n",
            "Epoch 914: | Loss: 0.18341 | Acc: 94.154\n",
            "Epoch 915: | Loss: 0.19698 | Acc: 91.692\n",
            "Epoch 916: | Loss: 0.19937 | Acc: 91.923\n",
            "Epoch 917: | Loss: 0.19766 | Acc: 93.154\n",
            "Epoch 918: | Loss: 0.16115 | Acc: 93.692\n",
            "Epoch 919: | Loss: 0.22379 | Acc: 90.385\n",
            "Epoch 920: | Loss: 0.20016 | Acc: 90.769\n",
            "Epoch 921: | Loss: 0.22354 | Acc: 91.231\n",
            "Epoch 922: | Loss: 0.20065 | Acc: 91.923\n",
            "Epoch 923: | Loss: 0.20656 | Acc: 90.231\n",
            "Epoch 924: | Loss: 0.18464 | Acc: 91.769\n",
            "Epoch 925: | Loss: 0.21046 | Acc: 91.538\n",
            "Epoch 926: | Loss: 0.20968 | Acc: 90.615\n",
            "Epoch 927: | Loss: 0.18337 | Acc: 91.769\n",
            "Epoch 928: | Loss: 0.23603 | Acc: 89.769\n",
            "Epoch 929: | Loss: 0.29208 | Acc: 91.538\n",
            "Epoch 930: | Loss: 0.27849 | Acc: 88.231\n",
            "Epoch 931: | Loss: 0.30704 | Acc: 86.000\n",
            "Epoch 932: | Loss: 0.28224 | Acc: 88.692\n",
            "Epoch 933: | Loss: 0.23678 | Acc: 91.000\n",
            "Epoch 934: | Loss: 0.20348 | Acc: 90.846\n",
            "Epoch 935: | Loss: 0.21784 | Acc: 90.923\n",
            "Epoch 936: | Loss: 0.26921 | Acc: 90.231\n",
            "Epoch 937: | Loss: 0.26499 | Acc: 88.077\n",
            "Epoch 938: | Loss: 0.26504 | Acc: 87.769\n",
            "Epoch 939: | Loss: 0.19140 | Acc: 91.769\n",
            "Epoch 940: | Loss: 0.19779 | Acc: 91.923\n",
            "Epoch 941: | Loss: 0.18888 | Acc: 92.308\n",
            "Epoch 942: | Loss: 0.19091 | Acc: 91.077\n",
            "Epoch 943: | Loss: 0.19070 | Acc: 91.462\n",
            "Epoch 944: | Loss: 0.21576 | Acc: 91.077\n",
            "Epoch 945: | Loss: 0.18568 | Acc: 92.846\n",
            "Epoch 946: | Loss: 0.23525 | Acc: 89.615\n",
            "Epoch 947: | Loss: 0.22894 | Acc: 90.385\n",
            "Epoch 948: | Loss: 0.22961 | Acc: 89.692\n",
            "Epoch 949: | Loss: 0.19027 | Acc: 92.385\n",
            "Epoch 950: | Loss: 0.25193 | Acc: 89.769\n",
            "Epoch 951: | Loss: 0.22297 | Acc: 91.231\n",
            "Epoch 952: | Loss: 0.20232 | Acc: 91.769\n",
            "Epoch 953: | Loss: 0.20937 | Acc: 90.462\n",
            "Epoch 954: | Loss: 0.17971 | Acc: 92.538\n",
            "Epoch 955: | Loss: 0.22423 | Acc: 89.000\n",
            "Epoch 956: | Loss: 0.21303 | Acc: 91.615\n",
            "Epoch 957: | Loss: 0.20168 | Acc: 91.538\n",
            "Epoch 958: | Loss: 0.20730 | Acc: 90.769\n",
            "Epoch 959: | Loss: 0.17745 | Acc: 92.769\n",
            "Epoch 960: | Loss: 0.15965 | Acc: 93.692\n",
            "Epoch 961: | Loss: 0.20468 | Acc: 90.231\n",
            "Epoch 962: | Loss: 0.19648 | Acc: 91.462\n",
            "Epoch 963: | Loss: 0.17778 | Acc: 92.846\n",
            "Epoch 964: | Loss: 0.19691 | Acc: 91.231\n",
            "Epoch 965: | Loss: 0.21738 | Acc: 89.385\n",
            "Epoch 966: | Loss: 0.22461 | Acc: 90.846\n",
            "Epoch 967: | Loss: 0.21939 | Acc: 87.923\n",
            "Epoch 968: | Loss: 0.18801 | Acc: 91.308\n",
            "Epoch 969: | Loss: 0.18224 | Acc: 91.154\n",
            "Epoch 970: | Loss: 0.18905 | Acc: 91.769\n",
            "Epoch 971: | Loss: 0.22006 | Acc: 91.154\n",
            "Epoch 972: | Loss: 0.22584 | Acc: 90.769\n",
            "Epoch 973: | Loss: 0.22700 | Acc: 90.846\n",
            "Epoch 974: | Loss: 0.23909 | Acc: 90.385\n",
            "Epoch 975: | Loss: 0.24897 | Acc: 89.385\n",
            "Epoch 976: | Loss: 0.18839 | Acc: 92.308\n",
            "Epoch 977: | Loss: 0.22900 | Acc: 90.385\n",
            "Epoch 978: | Loss: 0.21267 | Acc: 91.154\n",
            "Epoch 979: | Loss: 0.21347 | Acc: 90.769\n",
            "Epoch 980: | Loss: 0.20179 | Acc: 92.385\n",
            "Epoch 981: | Loss: 0.17768 | Acc: 93.000\n",
            "Epoch 982: | Loss: 0.18625 | Acc: 92.000\n",
            "Epoch 983: | Loss: 0.17441 | Acc: 92.462\n",
            "Epoch 984: | Loss: 0.21229 | Acc: 90.692\n",
            "Epoch 985: | Loss: 0.17656 | Acc: 92.154\n",
            "Epoch 986: | Loss: 0.19114 | Acc: 92.615\n",
            "Epoch 987: | Loss: 0.21089 | Acc: 90.231\n",
            "Epoch 988: | Loss: 0.18350 | Acc: 91.462\n",
            "Epoch 989: | Loss: 0.20782 | Acc: 90.692\n",
            "Epoch 990: | Loss: 0.17486 | Acc: 91.077\n",
            "Epoch 991: | Loss: 0.19416 | Acc: 89.769\n",
            "Epoch 992: | Loss: 0.21759 | Acc: 90.154\n",
            "Epoch 993: | Loss: 0.16436 | Acc: 93.231\n",
            "Epoch 994: | Loss: 0.20192 | Acc: 92.385\n",
            "Epoch 995: | Loss: 0.20405 | Acc: 91.308\n",
            "Epoch 996: | Loss: 0.25138 | Acc: 89.615\n",
            "Epoch 997: | Loss: 0.22136 | Acc: 91.769\n",
            "Epoch 998: | Loss: 0.20322 | Acc: 92.077\n",
            "Epoch 999: | Loss: 0.21887 | Acc: 90.692\n",
            "Epoch 1000: | Loss: 0.18802 | Acc: 92.154\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5sjURwbOG30"
      },
      "source": [
        "y_pred_list = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for X_batch in test_loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        y_test_pred = model(X_batch)\n",
        "        y_test_pred = torch.sigmoid(y_test_pred)\n",
        "        y_pred_tag = torch.round(y_test_pred)\n",
        "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
        "\n",
        "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACq2LtBuOLo6",
        "outputId": "7da583f0-b2eb-40a9-a5b3-7a8f9276047a"
      },
      "source": [
        "confusion_matrix(y_test, y_pred_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[88, 19],\n",
              "       [17, 30]])"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWV3pVmFOQxX",
        "outputId": "b92174cc-0a25-4425-c963-bac210e06e67"
      },
      "source": [
        "print(classification_report(y_test, y_pred_list))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.82      0.83       107\n",
            "           1       0.61      0.64      0.62        47\n",
            "\n",
            "    accuracy                           0.77       154\n",
            "   macro avg       0.73      0.73      0.73       154\n",
            "weighted avg       0.77      0.77      0.77       154\n",
            "\n"
          ]
        }
      ]
    }
  ]
}